{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Omdena-NeedEnergy \"Increasing Clean Energy Access in Africa\" project](https://omdena.com/projects/clean-energy-ai/)\n",
    "#### A.Montesino (Feb 10th, 2021)\n",
    "### Task X - XYZ\n",
    "\n",
    "References:\n",
    "* [Document #1](#http://www.loremipzum.com/en/)\n",
    "* [Document #2](#http://www.loremipzum.com/es/)\n",
    "* other...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.19.2\n",
      "Pandas version: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "# Install common libraries\n",
    "\n",
    "# NumPy\n",
    "try:\n",
    "    import numpy as np\n",
    "    # from numpy import *\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install numpy\n",
    "print( \"NumPy version:\", np.__version__ )\n",
    "\n",
    "# xlrd required by pandas' read_excel()\n",
    "try:\n",
    "    import xlrd \n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install xlrd\n",
    "\n",
    "# pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    #from pandas import *\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install pandas\n",
    "print( \"Pandas version:\", pandas.__version__ )    \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# maptplot\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    #from pandas import *\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install pandas\n",
    "\n",
    "# Seaborn\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install seaborn\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "Numexpr version:   2.7.1\n",
      "NumPy version:     1.19.2\n",
      "Python version:    3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Platform:          win32-AMD64-10.0.18362\n",
      "CPU vendor:        GenuineIntel\n",
      "CPU model:         Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\n",
      "CPU clock speed:   1992 MHz\n",
      "VML available?     True\n",
      "VML/MKL version:   Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "Number of threads used by default: 8 (out of 8 detected cores)\n",
      "Maximum number of threads: 64\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
     ]
    }
   ],
   "source": [
    "# pip list\n",
    "import numexpr\n",
    "numexpr.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-1.12.8-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth-httplib2\n",
      "  Using cached google_auth_httplib2-0.0.4-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Using cached httplib2-0.19.0-py3-none-any.whl (95 kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from google-api-python-client) (1.15.0)\n",
      "Collecting google-api-core<2dev,>=1.21.0\n",
      "  Using cached google_api_core-1.26.0-py2.py3-none-any.whl (92 kB)\n",
      "Collecting google-auth>=1.16.0\n",
      "  Using cached google_auth-1.26.1-py2.py3-none-any.whl (116 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing<3,>=2.4.2 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: pytz in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.1)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=14.3 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.24.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Using cached googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.1-py3-none-any.whl (36 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\alvar\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: httplib2, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-httplib2, uritemplate, protobuf, googleapis-common-protos, google-api-core, google-api-python-client, oauthlib, requests-oauthlib, google-auth-oauthlib\n",
      "Successfully installed cachetools-4.2.1 google-api-core-1.26.0 google-api-python-client-1.12.8 google-auth-1.26.1 google-auth-httplib2-0.0.4 google-auth-oauthlib-0.4.2 googleapis-common-protos-1.52.0 httplib2-0.19.0 oauthlib-3.1.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.1 uritemplate-3.0.1\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "# Enabling Jupyter notebook access to Google Drive\n",
    "\n",
    "# https://stackoverflow.com/questions/51571392/no-module-named-googleapiclient-discovery\n",
    "! pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "! pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to access Google Drive files directly from code, when running a non-Google Colab (i.e. Jupyter) notebook\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from tabulate import tabulate\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "# Remember after you run the script, you'll be prompted in your default browser to select your Google account\n",
    "# and permit your application for the scopes you specified earlier, don't worry, this will only happen the first time\n",
    "# you run it, and then token.pickle will be saved and will load authentication details from there instead.\n",
    "\n",
    "def get_gdrive_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    # return Google Drive API service\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "def get_size_format(b, factor=1024, suffix=\"B\"):\n",
    "    \"\"\"\n",
    "        Scale bytes to its proper byte format\n",
    "    e.g:\n",
    "        1253656 => '1.20MB'\n",
    "        1253656678 => '1.17GB'\n",
    "    \"\"\"\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\"]:\n",
    "        if b < factor:\n",
    "            return f\"{b:.2f}{unit}{suffix}\"\n",
    "        b /= factor\n",
    "    return f\"{b:.2f}Y{suffix}\"\n",
    "\n",
    "#  Since results is now a list of dictionaries, it isn't that readable, we pass items to this function \n",
    "# in order to print them in human readable format:\n",
    "def list_files(items):\n",
    "    \"\"\"given items returned by Google Drive API, prints them in a tabular way\"\"\"\n",
    "    if not items:\n",
    "        # empty drive\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        rows = []\n",
    "        for item in items:\n",
    "            # get the File ID\n",
    "            id = item[\"id\"]\n",
    "            # get the name of file\n",
    "            name = item[\"name\"]\n",
    "            try:\n",
    "                # parent directory ID\n",
    "                parents = item[\"parents\"]\n",
    "            except:\n",
    "                # has no parrents\n",
    "                parents = \"N/A\"\n",
    "            try:\n",
    "                # get the size in nice bytes format (KB, MB, etc.)\n",
    "                size = get_size_format(int(item[\"size\"]))\n",
    "            except:\n",
    "                # not a file, may be a folder\n",
    "                size = \"N/A\"\n",
    "            # get the Google Drive type of file\n",
    "            mime_type = item[\"mimeType\"]\n",
    "            # get last modified date time\n",
    "            modified_time = item[\"modifiedTime\"]\n",
    "            # append everything to the list\n",
    "            rows.append((id, name, parents, size, mime_type, modified_time))\n",
    "        print(\"Files:\")\n",
    "        # convert to a human readable table\n",
    "        table = tabulate(rows, headers=[\"ID\", \"Name\", \"Parents\", \"Size\", \"Type\", \"Modified Time\"])\n",
    "        # print the table\n",
    "        print(table)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Drive v3 API.\n",
    "    Prints the names and ids of the first 5 files the user has access to.\n",
    "    \"\"\"\n",
    "    service = get_gdrive_service()\n",
    "    # Call the Drive v3 API\n",
    "    results = service.files().list(\n",
    "        pageSize=5, fields=\"nextPageToken, files(id, name, mimeType, size, parents, modifiedTime)\").execute()\n",
    "    # get the results\n",
    "    items = results.get('files', [])\n",
    "    \n",
    "    # Since results is now a list of dictionaries, it isn't that readable, we pass items to this function in order to print them in human readable format:\n",
    "    # list all 20 files & folders\n",
    "    list_files(items)\n",
    "\n",
    "    # How can I check if key exists in list of dicts in python?\n",
    "    # https://stackoverflow.com/questions/14790980/how-can-i-check-if-key-exists-in-list-of-dicts-in-python\n",
    "    key ='text/csv'\n",
    "    print( any( key in d for d in items ) ) \n",
    "    [ i for i,d in enumerate( items ) if key in d ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pandas Profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/introduction.html)\n",
    "\n",
    "EDA using the pandas-profiling package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install and import required packages\n",
    "try:\n",
    "    from pandas_profiling import ProfileReport\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install pandas-profiling\n",
    "    \n",
    "def generateEDAreport_pandasProfile( p, sourceFileName ):\n",
    "    print( \"Generating Pandas Profiling EDA report for '%s'\" % sourceFileName )\n",
    "    profile = ProfileReport( p, title='Pandas Profiling Report', html={'style':{'full_width':True}} )\n",
    "\n",
    "    REPORT_DIRECTORY_PATH =os.path.join( os.path.split( sourceFileName )[ 0 ], \"EDA_reports\" )   # Currently not used\n",
    "    if not os.path.exists( REPORT_DIRECTORY_PATH ):\n",
    "        os.mkdir( REPORT_DIRECTORY_PATH )\n",
    "    print( \"Tentative destination report for reports '%s'\" % REPORT_DIRECTORY_PATH)  \n",
    "    \n",
    "    # Saving results to a HTML file\n",
    "    REPORT_SUFFIX =\"_pandasProfl.html\"\n",
    "    OUTPUT_FILE = os.path.join( REPORT_DIRECTORY_PATH, os.path.split( sourceFileName )[ 1 ]+REPORT_SUFFIX )\n",
    "    print( \"Outputfile:\", OUTPUT_FILE )\n",
    "    profile.to_file( OUTPUT_FILE )\n",
    "\n",
    "    # Outputting results inline, as part of the current notebook\n",
    "    # profile.to_notebook_iframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sweetviz](https://github.com/fbdesignpro/sweetviz)\n",
    "\n",
    "EDA using the SweetViz package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install and import required packages\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install pandas\n",
    "\n",
    "try:\n",
    "    import sweetviz as sv\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install sweetviz\n",
    "\n",
    "#EDA using Sweetviz\n",
    "def generateEDAreport_sweetViz( p, sourceFileName ):\n",
    "    print( \"Generating SweetViz EDA report for '%s'\" % sourceFileName )\n",
    "    sweet_report = sv.analyze( p )\n",
    "     \n",
    "    # Saving results to HTML file\n",
    "    \n",
    "    # Invocation without any arguments will generate a \"SWEETVIZ_REPORT.html\" output file, without any prefix linking it to the source file.\n",
    "    # sweet_report.show_html( )\n",
    "    REPORT_DIRECTORY_PATH =os.path.join( os.path.split( sourceFileName )[ 0 ], \"EDA_reports\" )   # Currently not used\n",
    "    if not os.path.exists( REPORT_DIRECTORY_PATH ):\n",
    "        os.mkdir( REPORT_DIRECTORY_PATH )\n",
    "    print( \"Tentative destination report for reports '%s'\" % REPORT_DIRECTORY_PATH)  \n",
    "\n",
    "    # Sending output to a specific destination directory other than working directory, however, requires invokation with explicit parameters \n",
    "    REPORT_SUFFIX =\"_sweetViz.html\"\n",
    "    OUTPUT_FILE = os.path.join( REPORT_DIRECTORY_PATH, os.path.split( sourceFileName )[ 1 ]+REPORT_SUFFIX )\n",
    "    print( \"Outputfile:\", OUTPUT_FILE )\n",
    "    \n",
    "    # sweet_report.show_html( filepath=OUTPUT_FILE, open_browser=False, layout='widescreen', scale=None )\n",
    "    sweet_report.show_html( filepath=OUTPUT_FILE, open_browser=False )\n",
    " \n",
    "    # Outputting results inline, as part of the current notebook\n",
    "    #sweet_report.show_notebook( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Autoviz](https://github.com/AutoViML/AutoViz/blob/master/README.md)\n",
    "\n",
    "EDA using the Autoviz package\n",
    "\n",
    "References:\n",
    "\n",
    "* [AutoViz: A New Tool for Automated Visualization, Dan Roth, Medium](https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad)  \n",
    "_An XGBoost model is repeatedly used to determine the most consistent set of features determined to be important by using a random set of features each time; the most prominent selected features can then serve to guide future plotting and visualization. ... To do this effectively, AutoViz classifies the selected variables as categorical, numerical, boolean, NLP text and so on in order to understand how to best plot them._  \n",
    "_Finally, using in-built heuristics, the tool will return the visuals deemed to have the greatest impact. AutoViz is also very much systematic: it uses all the selected variables with different chart types in order to deliver the best insights by letting the charts speak for themselves. ... AutoViz’ objective selection of features and plots can point data teams towards the best approaches using a systematic methodology and can greatly enhance a team’s productivity from the very outset of a project._  \n",
    "_Check [notebook AutViz_test.ipynb](https://github.com/DanRothDataScience/autoviz_test/blob/master/AutoViz_test.ipynb) for an example notebook._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install and import required packages\n",
    "# First install and import required packages\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    from autoviz.AutoViz_Class import AutoViz_Class\n",
    "except ImportError as e:\n",
    "    # module doesn't exist, deal with it.\n",
    "    ! pip install autoviz\n",
    "\n",
    "#EDA using Sweetviz\n",
    "def generateEDAreport_AutoViz( p, sourceFileName, targetVariable ):\n",
    "    print( \"Generating AutoViz EDA report for '%s'\" % sourceFileName )\n",
    "    sweet_report = sv.analyze( p )\n",
    "     \n",
    "    # Saving results to HTML file is apparently not possible\n",
    "    \n",
    "    REPORT_DIRECTORY_PATH =os.path.join( os.path.split( sourceFileName )[ 0 ], \"EDA_reports\" )   # Currently not used\n",
    "    if not os.path.exists( REPORT_DIRECTORY_PATH ):\n",
    "        os.mkdir( REPORT_DIRECTORY_PATH )\n",
    "    print( \"Tentative destination report for reports '%s'\" % REPORT_DIRECTORY_PATH)  \n",
    "\n",
    "    # Sending output to a specific destination directory other than working directory, however, requires invokation with explicit parameters \n",
    "    REPORT_SUFFIX =\"_AutoViz.html\"\n",
    "    OUTPUT_FILE = os.path.join( REPORT_DIRECTORY_PATH, os.path.split( sourceFileName )[ 1 ]+REPORT_SUFFIX )\n",
    "    print( \"Outputfile:\", OUTPUT_FILE )\n",
    "    \n",
    "    # EDA using AutoViz\n",
    "\n",
    "    # verbose option\n",
    "    #     if 0, display minimal information but displays charts on your notebook\n",
    "    #     if 1, print extra information on the notebook and also display charts\n",
    "    #     if 2, you will not see any charts but they will be quietly generated and save in your local current directory under the AutoViz_Plots directory\n",
    "    #           which will be created. Make sure you delete this folder periodically, otherwise, you will have lots of charts saved here if you used verbose=2 option a lot.\n",
    "    # OUTPUT_FILE =INPUT_FILE+\"/\"+\"AutoViz_Plots\"\n",
    "    # autoviz = AutoViz_Class().AutoViz( INPUT_FILE_PATH, verbose=0 )\n",
    "    autoviz = AutoViz_Class().AutoViz( \n",
    "        filename='',\n",
    "        sep=\",\",\n",
    "        depVar=targetVariable,\n",
    "        dfte= p,\n",
    "        header=0,\n",
    "        verbose=1,\n",
    "        lowess=False,\n",
    "        chart_format=\"svg\",\n",
    "        max_rows_analyzed=150000,\n",
    "        max_cols_analyzed=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\\Electricity_Accessibility_Areawise\\Access to electricity rural.zip\n",
      "File not among formats of interest\n",
      "C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\\Electricity_Accessibility_Areawise\\Access to electricity urban.zip\n",
      "File not among formats of interest\n",
      "C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\\Electricity_Accessibility_Areawise\\API_EG.ELC.ACCS.RU.ZS_DS2_en_csv_v2_1927758.csv_ilegible\n",
      "File not among formats of interest\n",
      "C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\\Electricity_Accessibility_Areawise\\API_EG.ELC.ACCS.UR.ZS_DS2_en_csv_v2_1927759.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 5, saw 66\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-0a967e149bb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\".csv\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\".xlsx\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\".xls\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 5, saw 66\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "# Loop recursively through directories, looking for relevant files type.\n",
    "DATA_ROOT_DIR = \"C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\"\n",
    "# DATA_ROOT_DIR = Path( \"C:/Users/alvar/OneDrive/Personal/Repositorio/Data Analytics/Omdena/NeedEnergy/Data\" )\n",
    "# DATA_ROOT_DIR = DATA_ROOT_DIR / \"NeedEnergy Meter Data/data for Mini Substation\"\n",
    "\n",
    "# DATA_ROOT_DIR = os.path.join( DATA_ROOT_DIR, \"NeedEnergy Meter Data/data for Mini Substation\")\n",
    "DATA_ROOT_DIR = os.path.join( DATA_ROOT_DIR, \"Electricity_Accessibility_Areawise\")\n",
    "\n",
    "\n",
    "#we shall store all the file names in this list\n",
    "filelist = []\n",
    "\n",
    "for root, dirs, files in os.walk( DATA_ROOT_DIR ):\n",
    "    for file in files:\n",
    "        #append the file name to the list\n",
    "        filelist.append( os.path.join( root,file ) )\n",
    "\n",
    "# For relevant file types, create a pandas dataframe with data.\n",
    "for fileName in filelist:\n",
    "    print( fileName )\n",
    "    if fileName.endswith( \".csv\" ):\n",
    "        p = pd.read_csv( fileName )\n",
    "        p.head( 3 )\n",
    "    elif ( fileName.endswith( \".xlsx\" ) or fileName.endswith( \".xls\" ) ):\n",
    "        p = pd.read_excel( fileName )\n",
    "        p.head( 3 )\n",
    "    else:\n",
    "        print( \"File not among formats of interest\" )\n",
    "        continue\n",
    "    \n",
    "    generateEDAreport_pandasProfile( p, fileName )\n",
    "    generateEDAreport_sweetViz( p, fileName )\n",
    "    # The third variable is the dependent variable, which AutoViz takes into account when creating visualizations\n",
    "    generateEDAreport_AutoViz( p, fileName, 'Consumption [kW]' )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
